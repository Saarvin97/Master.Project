# Master.Project

This project focuses on comparison of linguistic and machine learning model in fake news detection.The project duration was 5 months.Firstly, the data collection is separated in to 2 steps, one for collection of labelled data from various sources which includes Kaggle, articles and scraping from PolitiFact website. The seccond part is the data collection of Reddit data using Reddit API(PRAW). However, the reddit data would bnot have prelabelled data and the collection data contains only text. After collecting the data, data preprocessing for the labelled data is done. For data preprocesssing of text data, NLP is exeuted where NLTK library is used to lemmatize,stemming, removing stop words, punctutations, capital and lower case letters, tagging using part of speech,tokenizing,spell check and also removing URL. The cleaned data is stored in a separate data frame, lets say DF1.

Secondly, the data preprocessing is implemented for the Reddit data. The collection of data is done by using Reddit API PRAW where the secret key has been requested by creating an application. Reddit data is collected by using the subreddit names and stored in a dataframe. There are total of 12 dataframes collected and combined into single dataframe. The preprocessing of the data is done using the same NLTK library that has been used for labelled data collection.

After the cleaning of Reddit data the unsupervised learning is used to cluster the text into 2 class labels (0 and 1). Here Gaussian K-means clustering is implemented. Note that not only this but additionally DBSCAN and Agglomerative clustering also implemented but based on the clustering metrics these model does not show appropiriate results. After the clustering, several plotting has been done to visualize the clustered data and in order to know the representation of the label majority checking is done where majority of the text class label 0 is checked using internet searches and has been identified as FALSE label while 1 means TRUE label. The clustered has been evaluated using Calinski, Davies-Bouldin and Silhouette scores. When coming to model validation there are few things has been mentioned prior. Usually model validation is done by cross validation or manual checking for the news data. However in this case we dont have pre labelled data and manual checking thousands of rows of text data will take forever.The alternate way of validating this data is by implementing semi supervised learning. This is just to boost the confidence level of the predicted label used in future analysis and when ccomparing to cross validation this method is less accurate but it is an alternate way of validating the prediccted labels. The 20% of the Reddit data is being labelled manually and supervised learning has been implemented and the model is used to predict on the remaining 80% of the data. By comparing both classification report and if similar report has been achieved the model is able to predict well and we can use the new predicted label for future analysis. If similar report is not achieved the steps has been repeated using diffrent learning until we achieved similar report uisng the ensemble method where it combined 5 diffrent learning in one pipeline and the average prediction is obtained. After obtaining the similar report the data is used to retrained where the 20% labelled data is combined with the psedo labelled data which is the predicted label of the 80% data. This is known as self training method and this also undergone under a looping process. After achieving best predicted label the label is later combined with the other labelled data in single dataframe which contains 100,000 rows of cleaned and preprocessed text data with labels. Look at UNSUPERVISED_FRAMEWORK for more clear explanatiob.

The combined labelled data is later used for implementation of supervised and deep learning. Here the learning used are Logistic Regression, Naive Bayes Classifier, Support Vector Classifier, Gradient Boosting Clssifier, Decison Tree, Random Forest, Ensemble Methods, Sequential Word Embedding, Convolution 1D and Sentiment Analysis using subjecivity and polarity of the text. Note that linguistic in this report refers to grammatical,lexical and syntactic analysis. The performance are compared and analysed and in order to provide a better visualization a website is created where all the results from python has been imported into a website and the users able to compare the TRUE/FALSE text in the website provided.

Thank you for visitng my profile!
